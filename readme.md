# Projeto de Engenharia de Dados: Pipeline de ETL de Big Data com PySpark e Databricks

![Databricks](https://img.shields.io/badge/Databricks-Community-FF5722?logo=databricks)
![PySpark](https://img.shields.io/badge/Apache%20Spark-3.4-E25A1C?logo=apache-spark)
![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)

Este projeto demonstra a constru√ß√£o de um pipeline de ETL (Extra√ß√£o, Transforma√ß√£o e Carga) de ponta a ponta em um ambiente de **Big Data**, utilizando **PySpark** na plataforma **Databricks Community Edition**. O objetivo √© processar um grande volume de dados de corridas de t√°xi de Nova York, desde a ingest√£o at√© a an√°lise e o armazenamento em um formato otimizado.

Este projeto destaca habilidades em **Engenharia de Dados**, processamento distribu√≠do, otimiza√ß√£o de performance e resolu√ß√£o de problemas em ambientes de nuvem.

---

## üöÄ Arquitetura e Fluxo do Pipeline

O projeto foi desenvolvido inteiramente dentro de um Notebook no Databricks, seguindo estas etapas:

1.  **Extra√ß√£o (E):** Ingest√£o de dois datasets p√∫blicos: um arquivo Parquet com milh√µes de registros de corridas (`yellow_tripdata`) e um arquivo CSV com o mapeamento das zonas de t√°xi (`taxi_zone_lookup`).
2.  **Transforma√ß√£o (T):** Utiliza√ß√£o de **PySpark** para realizar uma s√©rie de transforma√ß√µes no DataFrame:
    *   Limpeza e renomea√ß√£o de colunas para padroniza√ß√£o.
    *   **JOINs** para enriquecer os dados das corridas com os nomes leg√≠veis das zonas de embarque e desembarque.
    *   **Engenharia de Features:** Cria√ß√£o de novas colunas de valor para o neg√≥cio, como `duracao_corrida_min`, `dia_da_semana` e `hora_do_dia`.
    *   Filtragem de dados inconsistentes para garantir a qualidade da an√°lise.
3.  **An√°lise (A):** Uso de **Spark SQL** para realizar consultas agregadas diretamente no DataFrame transformado, respondendo a perguntas de neg√≥cio como "Quais as rotas mais populares?" e "Qual o valor m√©dio das corridas por bairro?".
4.  **Carga (L):** O DataFrame final, limpo e enriquecido, √© salvo como uma **tabela gerenciada** no Databricks (`corridas_nyc_enriquecidas`), pronta para ser consumida por analistas de dados, cientistas de dados ou ferramentas de BI.

---

## üõ†Ô∏è Tecnologias Utilizadas

*   **Plataforma Cloud:** Databricks Community Edition
*   **Processamento de Big Data:** Apache Spark (via PySpark)
*   **Linguagem:** Python e Spark SQL
*   **Formatos de Dados:** Parquet, CSV

---

## üìà Principais Desafios e Aprendizados

*   **Configura√ß√£o de Ambiente:** Navegar pelas particularidades e limita√ß√µes da Databricks Community Edition, especialmente em rela√ß√£o ao Unity Catalog vs. Hive Metastore e as permiss√µes de escrita em diferentes sistemas de arquivos (DBFS vs. Volumes).
*   **Sintaxe do PySpark:** Adapta√ß√£o da l√≥gica de manipula√ß√£o de dados do Pandas para a sintaxe funcional e distribu√≠da do PySpark.
*   **Otimiza√ß√£o:** Entendimento pr√°tico de como o Spark executa as transforma√ß√µes de forma "pregui√ßosa" (lazy evaluation) e a import√¢ncia de salvar os resultados em formatos colunares como Parquet.

---

## ‚öôÔ∏è Como Reproduzir (Vis√£o Geral)

O c√≥digo completo est√° dispon√≠vel no arquivo `pipeline_nyc_taxi.py`. Para reproduzir, os passos gerais s√£o:
1.  Criar uma conta na Databricks Community Edition.
2.  Fazer o upload dos datasets para o ambiente Databricks.
3.  Criar um cluster e um notebook.
4.  Adaptar os caminhos dos arquivos no c√≥digo e executar as c√©lulas em ordem.

*Este projeto focou na l√≥gica de ETL e an√°lise, e n√£o na reprodutibilidade via clone, devido √†s particularidades do ambiente Databricks.*
